\section*{Variational Monte Carlo}
There are many good books that describe the basics of the VMC method, but I found the explanation in \cite{foulkes2001} to be particularly useful, and I have drawn heavily from this paper to build my understanding of VMC and DMC.
\subsubsection*{Monte Carlo Integration}
Before explaining variational Monte Carlo (VMC) I am going to describe the method for performing Monte Carlo integration. Assume that we are trying to integrate the function
\begin{equation}
  I = \int g(\R)d\R.
\end{equation}
This can be rewritten as
\begin{equation}
  I = \int f(\R)P(\R)d\R
\end{equation}
where $f(\R)=g(\R)/P(\R)$, and $P(\R)$ is the importance function (probability density). In our case $P(\R)$ can be interpreted as $\left<\psi_T|\psi_T\right>$ where $\left|\psi_T\right>$ is the trial wave function. Also note that $\R=(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_n)$, where $\mathbf{r}_i$ is the position of the ith particle, and a specific $\R$ is called a {\it walker}. Now you say that the integral looks like the expectation value of the random variable $f(\R)$ (Think of $\left< \hat{H} \right> = \int_{-\infty}^{\infty} \left<\psi\right|\hat{H}\left|\psi\right>$).

Now the integral can be determined by drawing an infinite number of samples from $f(\R)$ and computing the average.
\begin{equation}
  I = \lim_{N \to \infty} \frac{1}{N}\sum\limits_{n=1}^N f(\R_n)
\end{equation}
The integral can then be approximated by
\begin{equation}
  I \approx \frac{1}{N} \sum\limits_{n=1}^N f(\R_n).
\end{equation}
Notice that by the central limit theorem if $f(\R)$ has mean $\mu$ and standard deviation $\sigma^2$ then $I$ will have mean $\mu$ and standard deviation $\sigma/\sqrt{N}$.

\subsubsection*{The Metropolis Algorithm}
Often times sampling from the distribution $P(\R)$ is difficult because $P(\R)$ is complicated and difficult to invert. The metropolis algorithm allows us to sample complicated $P(\R)$'s. The steps are as follows.
\begin{enumerate}
  \item Start at some random walker $\R$.
  \item Propose a move to a new position $\R'$, pulled for a distribution $T(\mathbf{R \leftarrow R'})$.
  \item The probability of accepting the move is given by
    \begin{equation}
      A(\mathbf{R' \leftarrow R}) = \mathrm{min}\left( 1, \frac{T(\mathbf{R' \leftarrow R}) P(\R'))}{T(\mathbf{R' \leftarrow R}) P(\R))} \right).
    \end{equation}
    $T(\mathbf{R' \leftarrow R})$ can be 1 or a Gaussian centered around the current walker, or something else entirely. A random number is then pulled from $r=U(0,1)$, and the move is accepted if $r<A(\mathbf{R' \leftarrow R})$.
  \item Repeat from step 2.
\end{enumerate}

\subsubsection*{VMC}
To do VMC you first start with a trail wave function $\Psi_T$. The accuracy of VMC is sensitive to the choice of $\Psi_T$. The trial wave function is then used to find a rigid upper bound on the ground state energy, $E_0$.
\begin{equation}
  E_V = \frac{\int \Psi_T^*(\R)\hat{H}\Psi_T(\R)d\R}{\int \Psi_T^*(\R)\Psi_T(\R)d\R} \le E_0
\end{equation}
The methods above are used to evaluate this integral which can be written as
\begin{equation}
  E_V = \frac{\int |\Psi_T(\R)|^2 [\Psi_T^{-1}(\R)\hat{H}\Psi_T(\R)d\R]}{\int |\Psi_T(\R)|^2 d\R}.
\end{equation}
Random walkers are generated, $\{\R_n: n=1,N\}$, from the distribution $P(\R) = |\Psi_T(\R)|^2/\int|\Psi_T(\R)|^2d\R$. The local energy at each point is found using $E_L(\R) = \Psi_T^{-1}(\R) \hat{H} \Psi_T(\R)$. Now rewriting $E_V$ in terms of $E_L$ we get
\begin{equation}
  E_V = \int P(\R) E_L(\R) d\R.
\end{equation}
The local energy can then be sampled using the Metropolis method described above to give the energy.
\begin{equation}
  E_V \approx \frac{1}{N} \sum\limits_{n=1}^N E_L({\R_n}).
\end{equation}

At this point certain parameters in $\Psi_T$ can be varied until a minimum in the energy if found. A minimum in the energy will be produced when $\Psi_T \rightarrow \Psi_0$.
