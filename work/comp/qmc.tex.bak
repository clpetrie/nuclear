\subsection{Quantum Monte Carlo}
Many methods that use the variational principle often start with a trial wave function similar to the one used in HF, sometimes with Jastrow-like correlations \cite{jastrow1955}. However, to get an upper bound on energies they usually have to solve some some sort of differential equation or multi-dimensional integrals. Especially if complicated spin-isospin dependent correlations are applied to the trial wave function these integrals can be quite difficult to solve. Quantum Monte Carlo (QMC) calculations use statistical sampling to solve large integrals that would otherwise be intractable. There are many good resources from which to learn about the many different QMC methods, \cite{kalos1962,schmidt1999,foulkes2001,carlson2015}. Here I will describe the methods that I have used in this dissertation, those being Variational, Diffusion, and Auxiliary Field Diffusion Monte Carlo methods. Before describing those methods I am going to introduce the ideas of Monte Carlo integration and the Metropolis algorithm. Both of these tools will be used in the various QMC techniques.

\subsubsection{Monte Carlo Integration}
I will illustrate Monte Carlo Integration by imagining that we want to integrate the function $g(\mathbf{R})$ where $\mathbf{R}=\mathbf{r}_1,\mathbf{r}_2,\ldots,\mathbf{r}_n$i, so 
\begin{equation}
   I=\int g(\mathbf{R}) d\mathbf{R}.
\end{equation}
We can rewrite this integral in terms of a probability density called an importance function $P(\mathbf{R})$, and $f(\mathbf{R}) = g(\mathbf{R})/P(\mathbf{R})$.
\begin{equation}
   I=\int f(\mathbf{R}) P(\mathbf{R}) d\mathbf{R}
   \label{equ:mci}
\end{equation}
This integral looks like the expectation value of $f(\R)$ as long as the $\R$ are distributed according to $P(\R)$. Thus we can find the integral by finding the expectation value. One way to do that would be to take an infinite number of samples configurations from the importance function and evaluate the average of $f(\R)$ given that infinite set of samples.
\begin{equation}
   I=\lim\limits_{N\rightarrow\infty} \frac{1}{N} \sum\limits_{n=1}^N f(\mathbf{R}_n)
\end{equation}
Since we can't take an infinite number of samples we can approximate the integral by taking a large number of samples and finding the an approximate expectation value from that finite set.
\begin{equation}
   I \approx \frac{1}{N} \sum\limits_{n=1}^N f(\mathbf{R}_n).
\end{equation}

This method of integration is useful especially when the dimensions of the integration increase. In many-body quantum mechanics the dimension of the integrals can be quite large, including several dimensions for each particle in the calculation. Monte Carlo Integration only needs to sample each of these dimension decreasing the work required by a substantial amount.

\subsubsection{Metropolis Algorithm}
Let's assume that we are doing Monte Carlo Integration as described above, but the importance function, $P(\mathbf{R})$, from which the random variables $\mathbf{R}$ are drawn is a simple invertible function. We can then simply generate a random variable from the uniform distribution, $u$, and then figure out what value of $x$ gives us that value given $P(x)=u$. This works quite well assuming the function $P(x)$ is invertible. But let's say that we can't invert it. This is where the Matropolis algorithm comes in, allowing us to get random samples from $P(x)$ even if it's noninvertible. The Metropolis algorithm is a Markov Chain method that uses only the previous point to determine where the next move will be, i.e. the step doesn't depend on history other than the previous step. These are the steps to the algorithm.
\begin{enumerate}
   \item Start at a random position, $\mathbf{R}$.
   \item Propose a move to a new position $\mathbf{R}'$, pulled from a distribution $T(\mathbf{R}'|\mathbf{R})$, where $T$ can be a Gaussian centered on the current position. This makes sure that too large of steps aren't taken.
   \item The probability of accepting the move is given by
   \begin{equation}
      A(\mathbf{R}'|\mathbf{R}) = \frac{P(\mathbf{R}')}{P(\mathbf{R})}
   \end{equation}
   \item If $A\ge1$, then the move is accepted. Otherwise a random number, $u$,is generated from a uniform distribution between 0 and 1, and the move is accepted if $A>u$.
\end{enumerate}

\subsubsection{Variational Monte Carlo}
Variational Monte Carlo (VMC) starts with a trial wave function, $\psi_T$, that should have some non-zero overlap with the actual ground state wave function, and a Hamiltonian, $\hat{H}$. The expectation value of the Hamiltonian in the trial state gives what is called the variational energy. By the variational principle this energy is guaranteed to be an upper bound on the true ground state wave function.
\begin{equation}
   E_V = \frac{\int\psi_T^*(\mathbf{R})\hat{H}\psi_T(\mathbf{R})d\mathbf{R}}{\int\psi_T^*(\mathbf{R})\psi_T(\mathbf{R})d\mathbf{R}} \le E_0
\end{equation}

We want to do this integral using Monte Carlo integration and so we need it to look like equation \ref{equ:mci}. One way to get it into this form is to multiply the top integrand by $\psi_T(\mathbf{R})\psi_T^{-1}(\mathbf{R})$ which gives
\begin{equation} 
  E_V = \int P(\mathbf{R}) E_L(\mathbf{R}) d\mathbf{R},
\end{equation} 
where $P(\mathbf{R}) = |\Psi_T(\mathbf{R})|^2/\int|\Psi_T(\mathbf{R})|^2d\mathbf{R}$ is the importance function and $E_L(\mathbf{R}) = \Psi_T^{-1}(\mathbf{R}) \hat{H} \Psi_T(\mathbf{R})$ is called the local energy.

Now using the metropolis algorithm we can draw a set of random configurations, $\{\mathbf{R}_n: n=1,N\}$ from the probability distribution $P(\R)$ and use those to sample the local energy. These random configurations are called walkers and contain the positions as well as spins and isospins of each particle. The variational energy is them approximated by the expectation value of those samples.
\begin{equation} 
  E_V \approx \frac{1}{N} \sum\limits_{n=1}^N E_L({\mathbf{R}_n}). 
\end{equation} 

At this point certain parameters in $\Psi_T$ can be varied until a minimum in the energy is found. A minimum in the energy will be produced when $\Psi_T \rightarrow \Psi_0$. It is important to note however that the trial wave functions that we often use are not exactly the ground state wave functions and so the energies that we produce are only the minimum energy for that specific trial wave function. That is why it is important to start with the best trial wave function possible.

\subsubsection{Diffusion Monte Carlo}
Diffusion Monte Carlo (DMC) solves for the ground state by letting the walkers diffuse in imaginary time. You start with the Schr\"odinger equation
\begin{equation}
   \hat{H}\Psi = i\hbar\frac{\partial\Psi}{\partial t}.
\end{equation}
Now we substitute time for imaginary time using $\tau=it/\hbar$ and notice that this looks similar to the diffusion equation.
\begin{equation}
   \hat{H}\Psi = -\frac{\partial\Psi}{\partial\tau}
\end{equation}
At this point we assume that the solution will consist of exponentials, but we shift the energies by a parameter, $E_0$ that we can use to control the normalization, $V\rightarrow V - E_0$ and $E_n \rightarrow E_n-E_0$.
\begin{equation}
   \Psi(\mathbf{R},\tau) = \sum\limits_{n=0}^\infty c_n\phi_n(\mathbf{R})e^{-\tau(E_n-E_0)}
\end{equation}
Then one of the key parts of DMC is that as you let $\tau\rightarrow\infty$ all of the states higher than the ground state die because the difference $E_n-E_0$ is non-zero and the intinity $\tau$ kills the exponential. This leaves only the ground state.
\begin{equation}
   \lim\limits_{\tau\rightarrow\infty}\Psi(\mathbf{R},\tau) = c_0\phi_0(\mathbf{R})
\end{equation}

Since we cannot generally compute this limit, $\lim\limits_{\tau\rightarrow\infty}\Psi(\mathbf{R},\tau) = \lim\limits_{\tau\rightarrow\infty}\Psi(\mathbf{R})e^{(-H-E_0)\tau}$, directly we split the propogation into small steps in imaginary time. To do this let's write the propogated wave function and insert a complete set of states.
\begin{equation}
   \braket{\R'}{\Psi_T(\tau)} = \int d\R \bra{\R'}e^{-(H-E_0)\tau}\ket{\R}\braket{\R}{\Psi_T(0)}
\end{equation}
Now you can break $\tau$ up into $N$ smaller time steps $\Delta \tau = \tau/N$ and insert a complete set of states between each finite time propagator,
\begin{align}
   \braket{\R_N}{\Psi_T(\tau)} &= \int d\R_1 \ldots d\R_N \left[\prod\limits_{i=1}^N \bra{\R_i}e^{-(H-E_0)\Delta\tau}\ket{\R_{i-1}}\right] \braket{\R_0}{\psi_t(0)} \\
   &= \int d\R_1 \ldots d\R_N \left[\prod\limits_{i=1}^N G(\R_i,\R_{i-1},\Delta\tau)\right] \braket{\R_0}{\psi_t(0)},
\end{align}
where $\R_N=\R'$, $\R_0=\R$ and $G(\R',\R,\tau)=\bra{\R'}e^{-(H-E_0)\tau}\ket{\R}$, is often called the Green's function or the propagator. It is often convenient to split up the kinetic and potential pieces of the Greens function into two parts. The kinetic term is used to move the walkers and the potential part is used to speed up convergence via a branching algorithm. The kinetic term gives us
\begin{equation}
   G_0(\R',\R,\Delta \tau) = \bra{\R'}e^{-T\Delta \tau}\ket{\R},
\end{equation}
which can be written as a diffusion term
\begin{equation}
   G_0(\R',\R,\Delta \tau) = \left(\frac{m}{2\pi\hbar^2\Delta\tau}\right)^{3A/2}e^{-m(\R'-\R)^2/2\hbar^2\Delta\tau}.
\end{equation}
The piece that contains the potential then can be used to give a weight that gets used with the branching algorithm.
\begin{equation}
   w(\R') = \bra{\R'}e^{-(V-E_0)\Delta\tau}\ket{\R}
\end{equation}

This sampling can give us large uncertainties and so we often use a function that does a good job describing the system (this can be the trial wave function) to guide the sampling. This function is called an importance function, $\Psi_I(\R)$, and thus the method is called importance sampling. The idea is that instead of sampling directly from the Green's function you will sample from
\begin{equation}
   G(\R',\R,\Delta\tau)\frac{\braket{\R}{\Psi_I}}{\braket{\R'}{\Psi_I}}.
\end{equation}
It turns out that this greatly improves the variance of the sampling. The DMC algorithm can generally be written as follows.
\begin{enumerate}
   \item Generate a set of random walkers. These can be random or from a previously done VMC calculation.
   \item For each walker propose a move, $\R' = \R + \chi$, where $\chi$ is a random number from the shifted Gaussian $\exp\left(\frac{m}{2\hbar^2\Delta\tau}\left(\R'-\R+2\frac{\nabla\Psi_I(\R')}{\Psi_I(\R')}\right)^2\right)$.
   \item The move is then accepted with the probability $P_{acc}(\R'\leftarrow\R)=\frac{\Psi_T^2(\R')}{\Psi_T^2(\R)}$.
   \item For each walker calculate the weight $w(\R')=\exp\left(-\left(\frac{E_L(\R')+E_L(\R)}{2}-E_0\right)\Delta\tau\right)$.
   \item Do branching. This means that the number of copies for each walker that continues in the calculation is given by $\mathrm{int}(w(\R')+\chi)$, where $\chi$ is a uniform random number from $[0,1]$. This way walkers with a small weight will more often be removed from the calculation and talkers with high weights will multiply.
   \item Calculate and collect the observables and uncertainties needed and increase the imaginary time by $\Delta\tau$.
   \item Repeat from step 2 to 6 until the uncertainties are small enough.
\end{enumerate}

This method is described well in \cite{carlson2015} and \cite{foulkes2001}. Green's function Monte Carlo (GFMC) and auxiliary field diffusion Monte Carlo (AFDMC) but use this meathod to handle the spatial part of the calculation but they each handle the spin part differently. Everywhere where the trial wave function is used in GFMC there is an explicit sum over the spin and isospin states. The number of possible spin states given $A$ nucleons is $2^A$. The number of isospin states given $A$ nucleons and $Z$ protons can be reduced to ${A \choose Z}$ states leaving us with a total of $2^A {A \choose Z}$ spin-isospin states. The number of states increases quickly as the number of nucleons increases. As a result to date the largest nuclei that can be calculated with GFMC is ${}^{12}C$. As a results of this AFDMC was developed as a way to use Monte Carlo methods to sample the number of states.

\subsubsection{Auxiliary Field Diffusion Monte Carlo}
\label{sec:AFDMC}
To overcome the exponentially large number of spin-isospin states that have to be summed over in GFMC the AFDMC was developed in 1999 \cite{schmidt1999} to sample these states and, in analogy to moving the position of each walker, rotate the spin and isospin of each walker. To do this we use the basis where an element of the basis contains the three spatial coordinates of each particle and the amplitude for each particle to be in each of the four possible spin-isospin states $\ket{s} = \ket{p\uparrow,p\downarrow,n\uparrow,n\downarrow}$. Now we need to write the spin-isospin dependent part of the propagator in this basis,
\begin{equation}
   G_{SD}(R'S',RS,\dt) = \bra {R'S'}e^{-V_{SD}\dt} \ket{RS},
\end{equation}
where $V_{SD}$ does not include the central, non spin-isospin part of the potential. Here the potential can be written as
\begin{equation}
   V_{SD} = \sum\limits_{p=2}^{M}\sum\limits_{i<j} \vpij\Opij,
\end{equation}
where M is the number of operators that you use(\textit{i.e.} $M=6$ for the $v_6$ potential or $M=18$ for the Argonne $v_{18}$ two-body potential \cite{wiringa1984}). In this study we have used the standard $v_6$ potential which includes the operators $\si\cdot\sj$, $\ti\cdot\tj$, $\si\cdot\sj \ti\cdot\tj$, $S_{ij}$ and $S_{ij} \ti\cdot\tj$, where $S_{ij} = 3\si\cdot\hat{r}_{ij}\sj\cdot\hat{r}_{ij}-\si\cdot\sj$. Here the sigmas and taus are the pauli matrices applied to spin and isospin respectively. This potential can be written in the more convenient form
\begin{equation}
\begin{split}
   V_{SD} &= \frac{1}{2}\sum\limits_{i,\alpha,j,\beta} \sigma_{i,\alpha}A^{\sigma}_{i,\alpha,j,\beta}\sigma_{j,\beta} \\
   &+ \frac{1}{2}\sum\limits_{i,\alpha,j,\beta} \sigma_{i,\alpha}A^{\sigma\tau}_{i,\alpha,j,\beta}\sigma_{j,\beta}\ti\cdot\tj \\
   &+ \frac{1}{2}\sum\limits_{i,j} A^{\tau}_{i,j}\ti\cdot\tj,
\end{split}
\label{equ:VwithA}
\end{equation}
where we have defined these new $A$ matrices. These matrices are written in terms of the $\vpij$ parameters above. For example the simplest matrix is the tau matrix which it is easy to see is $A^{\tau}_{i,j} = v_{\tau}(r_{ij})$. The half in equation~\ref{equ:VwithA} is because the sums are now over all particles $i$ and $j$ instead of over the pairs. These matrices are zero when $i=j$ and they are symmetric. We can also write these matrices in terms of their eigenvalues and eigenvectors.
\begin{align}
   &\sum\limits_{j,\beta} A^{\sigma}_{i,\alpha,j,\beta}\psi^{\sigma}_{n,j,\beta} = \lambda^{\sigma}_n\psi^{\sigma}_{n,i,\alpha} \\
   &\sum\limits_{j,\beta} A^{\sigma\tau}_{i,\alpha,j,\beta}\psi^{\sigma\tau}_{n,j,\beta} = \lambda^{\sigma\tau}_n\psi^{\sigma\tau}_{n,i,\alpha} \\
   &\sum\limits_{j} A^{\tau}_{i,j}\psi^{\tau}_{n,j} = \lambda^{\tau}_n\psi^{\tau}_{n,i}
\end{align}
Written in terms of these eigenvalues and eigenvectors the potential can be written as
\begin{equation}
\begin{split}
   V_{SD} &= \frac{1}{2}\sum\limits_{n=1}^{3A} \left(O_{n}^{\sigma}\right)^2 \lambda_n^{\sigma} \\
   &+ \frac{1}{2}\sum\limits_{\alpha=1}^{3}\sum\limits_{n=1}^{3A} \left(O_{n\alpha}^{\sigma\tau}\right)^2 \lambda_n^{\sigma\tau} \\
    &+ \frac{1}{2}\sum\limits_{\alpha=1}^{3}\sum\limits_{n=1}^{A} \left(O_{n\alpha}^{\tau}\right)^2 \lambda_n^{\tau},
\end{split}
\end{equation}
where the operators are given by
\begin{equation}
\begin{split}
   O_{n}^{\sigma} &= \sum\limits_{j,\beta} \sigma_{j,\beta}\psi_{n,j,\beta}^{\sigma} \\
   O_{n\alpha}^{\sigma\tau} &= \sum\limits_{j,\beta} \tau_{j,\alpha}\sigma_{j,\beta}\psi_{n,j,\beta}^{\sigma\tau} \\
   O_{n\alpha}^{\tau} &= \sum\limits_{j} \tau_{j,\alpha}\psi_{n,j}^{\tau}
\end{split}
\end{equation}
These operators in the propagator now have the effect of rotating the spinors, analogous to diffusing the walkers in space. To reduce the order of the operators in the propagator from quadratic to linear we now use the Hubbard-Stratanovich transformation.
\begin{equation}
   e^{-\frac{1}{2}\lambda\hat{O}^2} = \frac{1}{\sqrt{2\pi}} \int dx e^{-\frac{x^2}{2} + \sqrt{-\lambda}x\hat{O}}
\end{equation}
The variable $x$ is called an auxiliary field. Using the fact that there are $3A$ $O_{n}^{\sigma}$ operators, $9A$ $O_{n\alpha}^{\sigma\tau}$ operators and $3A$ $O_{n\alpha}^{\tau}$ operators, for a total of $15A$ operators, and by using the Hubbard-Stratanovich transformation we can write the spin-isospin dependent part of the propagator as
\begin{equation}
   \prod\limits_{n=1}^{15A}\frac{1}{\sqrt{2\pi}}\int dx_n e^{-\frac{x_n^2}{2}}e^{\sqrt{-\lambda_n\dt} x_nO_n}.
   \label{equ:GSD}
\end{equation}
The spinors are then rotated based on auxiliary fields sampled from the Gaussian with unit variance in equation~\ref{equ:GSD}.
